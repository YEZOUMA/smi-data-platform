# üöÄ Guide de D√©ploiement - Plateforme SMI

## Table des Mati√®res
1. [Pr√©requis](#pr√©requis)
2. [Installation Rapide](#installation-rapide)
3. [Configuration D√©taill√©e](#configuration-d√©taill√©e)
4. [D√©marrage des Services](#d√©marrage-des-services)
5. [V√©rification](#v√©rification)
6. [Troubleshooting](#troubleshooting)

## Pr√©requis

### Syst√®me
- **OS**: Linux (Ubuntu 20.04+), macOS, ou Windows avec WSL2
- **RAM**: Minimum 8 GB (16 GB recommand√©)
- **Disque**: 20 GB minimum d'espace libre
- **CPU**: 4 c≈ìurs minimum

### Logiciels
```bash
# Docker
docker --version  # Doit √™tre >= 24.0.0
docker-compose --version  # Doit √™tre >= 2.0.0

# Python
python3 --version  # Doit √™tre >= 3.11

# Git
git --version
```

## Installation Rapide

### 1. Cloner le Projet
```bash
git clone <repository-url>
cd smi-data-platform
```

### 2. Configuration Initiale
```bash
# Cr√©er l'environnement virtuel Python
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# ou: venv\Scripts\activate  # Windows

# Installer les d√©pendances
pip install -e ".[dev]"

# Configuration de base
make setup
```

### 3. Configurer les Variables d'Environnement
```bash
# Copier le fichier d'exemple
cp .env.example .env

# √âditer .env avec vos param√®tres
nano .env  # ou vim, code, etc.
```

**Variables Critiques √† Modifier:**
```bash
# S√©curit√© - CHANGER EN PRODUCTION !
AIRFLOW__WEBSERVER__SECRET_KEY=<g√©n√©rer-une-cl√©-forte>
SUPERSET_SECRET_KEY=<g√©n√©rer-une-cl√©-forte>
POSTGRES_PASSWORD=<mot-de-passe-fort>

# Email pour notifications
NOTIFICATION_EMAIL_LIST=votre-email@example.com
```

### 4. Lancer l'Infrastructure
```bash
# D√©marrer tous les services
make up

# Initialiser Airflow (premi√®re fois uniquement)
make airflow-init

# Importer les donn√©es sources
make import-data
```

### 5. Acc√©der aux Services

Les services seront disponibles aux URLs suivantes:

| Service | URL | Credentials |
|---------|-----|-------------|
| **Airflow** | http://localhost:8080 | admin / admin |
| **Superset** | http://localhost:8088 | admin / admin |
| **Grafana** | http://localhost:3000 | admin / admin |
| **Prometheus** | http://localhost:9090 | - |
| **MinIO Console** | http://localhost:9001 | minioadmin / minioadmin |

## Configuration D√©taill√©e

### Configuration PostgreSQL

Le Data Warehouse est automatiquement initialis√© avec:
- Sch√©mas: bronze, silver, gold, metadata
- Tables dimensionnelles et de faits
- Indexes et contraintes
- Fonctions utilitaires

**Connexion manuelle:**
```bash
make db-shell
# ou
psql -h localhost -p 5433 -U smi_user -d smi_dwh
```

### Configuration Airflow

**Connexions √† configurer dans l'interface Airflow:**

1. **PostgreSQL DWH** (`postgres_dwh`)
   - Conn Type: `Postgres`
   - Host: `postgres-dwh`
   - Schema: `smi_dwh`
   - Login: `smi_user`
   - Password: `smi_password`
   - Port: `5432`

2. **MinIO S3** (`minio_s3`)
   - Conn Type: `Amazon Web Services`
   - Extra: 
   ```json
   {
     "aws_access_key_id": "minioadmin",
     "aws_secret_access_key": "minioadmin",
     "host": "http://minio:9000"
   }
   ```

### Configuration Superset

**Connexion au Data Warehouse:**

1. Acc√©der √† Superset: http://localhost:8088
2. Aller dans Settings > Database Connections
3. Ajouter une connexion:
   - Database: `SMI Data Warehouse`
   - SQLAlchemy URI: 
     ```
     postgresql://smi_user:smi_password@postgres-dwh:5432/smi_dwh
     ```

### Configuration dbt

Le projet dbt est dans `/dbt` avec la structure:
```
dbt/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ bronze/     # Sources brutes
‚îÇ   ‚îú‚îÄ‚îÄ silver/     # Transformations
‚îÇ   ‚îî‚îÄ‚îÄ gold/       # Analytics
‚îú‚îÄ‚îÄ tests/          # Tests data quality
‚îî‚îÄ‚îÄ dbt_project.yml
```

**Ex√©cuter dbt:**
```bash
make dbt-run      # Ex√©cuter transformations
make dbt-test     # Ex√©cuter tests
make dbt-docs     # G√©n√©rer documentation
```

## D√©marrage des Services

### Ordre de D√©marrage Recommand√©

1. **Infrastructure de Base**
   ```bash
   docker-compose up -d postgres-dwh postgres-airflow redis minio
   ```

2. **Airflow**
   ```bash
   docker-compose up -d airflow-init
   docker-compose up -d airflow-webserver airflow-scheduler
   ```

3. **Analytics & Monitoring**
   ```bash
   docker-compose up -d superset prometheus grafana
   ```

### V√©rification du D√©marrage

```bash
# V√©rifier les services
make ps

# V√©rifier la sant√©
make health

# Logs en temps r√©el
make logs
```

## V√©rification

### 1. V√©rifier PostgreSQL
```bash
make db-shell

# Dans psql:
\dt bronze.*      # Tables Bronze
\dt silver.*      # Tables Silver
\dt gold.*        # Tables Gold

# Compter les enregistrements
SELECT COUNT(*) FROM bronze.smi_raw;
```

### 2. V√©rifier Airflow
1. Ouvrir http://localhost:8080
2. V√©rifier que le DAG `smi_full_pipeline` est visible
3. L'activer (toggle ON)
4. D√©clencher manuellement : "Trigger DAG"

### 3. V√©rifier Superset
1. Ouvrir http://localhost:8088
2. Aller dans "Datasets"
3. Ajouter les tables gold.*
4. Cr√©er un dashboard de test

### 4. V√©rifier Grafana
1. Ouvrir http://localhost:3000
2. V√©rifier la connexion √† Prometheus
3. Importer les dashboards pr√©-configur√©s

## Ex√©cution du Pipeline

### Ex√©cution Manuelle
```bash
# Pipeline complet
make run-pipeline

# Suivre l'ex√©cution
make logs-airflow
```

### Ex√©cution Programm√©e
Le pipeline s'ex√©cute automatiquement:
- **Fr√©quence**: Quotidien √† 2h00 AM
- **Dur√©e estim√©e**: 15-20 minutes
- **SLA**: 4 heures

### Monitoring de l'Ex√©cution
- **Airflow UI**: http://localhost:8080/dags/smi_full_pipeline
- **Grafana**: http://localhost:3000 (dashboard "Pipeline Health")
- **Logs**: `make logs-airflow`

## Troubleshooting

### Probl√®me: Services ne d√©marrent pas

**Solution 1: V√©rifier Docker**
```bash
docker info
docker-compose version
```

**Solution 2: Lib√©rer les ports**
```bash
# V√©rifier les ports utilis√©s
sudo lsof -i :8080  # Airflow
sudo lsof -i :8088  # Superset
sudo lsof -i :5433  # PostgreSQL

# Arr√™ter les processus conflictuels
```

**Solution 3: Augmenter les ressources Docker**
- Dans Docker Desktop: Settings > Resources
- RAM: Minimum 8 GB
- Swap: 2 GB

### Probl√®me: Airflow ne peut pas se connecter √† PostgreSQL

**Solution:**
```bash
# Recr√©er les conteneurs
docker-compose down
docker-compose up -d postgres-dwh
sleep 10  # Attendre que PostgreSQL soit pr√™t
docker-compose up -d airflow-webserver airflow-scheduler
```

### Probl√®me: Pipeline √©choue

**Diagnostic:**
```bash
# Voir les logs d√©taill√©s
make logs-airflow

# Acc√©der au conteneur
docker exec -it smi-airflow-scheduler bash

# V√©rifier les donn√©es sources
ls -lh /opt/airflow/data/source/

# Tester l'extraction manuellement
cd /opt/airflow
python -m src.extract.excel_extractor /opt/airflow/data/source/Donnees_POC2024_2025_10122025.xls
```

### Probl√®me: Donn√©es manquantes

**V√©rifications:**
```bash
# V√©rifier chaque couche
make db-shell

# Bronze
SELECT COUNT(*) FROM bronze.smi_raw;
SELECT * FROM bronze.smi_raw LIMIT 5;

# Silver  
SELECT COUNT(*) FROM silver.smi_cleaned;
SELECT * FROM silver.smi_cleaned LIMIT 5;

# Gold
SELECT COUNT(*) FROM gold.fait_deces_neonatals;
```

### Probl√®me: Performances lentes

**Optimisations:**
```bash
# 1. Augmenter les workers Airflow
# √âditer docker-compose.yml:
AIRFLOW_PARALLELISM=64
AIRFLOW_MAX_ACTIVE_TASKS=32

# 2. Optimiser PostgreSQL
# √âditer docker-compose.yml:
POSTGRES_SHARED_BUFFERS=512MB
POSTGRES_WORK_MEM=64MB

# 3. Red√©marrer
docker-compose restart
```

## Maintenance

### Sauvegardes Automatiques
```bash
# Configurer dans .env:
BACKUP_ENABLED=True
BACKUP_SCHEDULE="0 2 * * *"  # 2h AM

# Sauvegarder manuellement
make db-backup

# Restaurer
make db-restore
```

### Nettoyage
```bash
# Nettoyer les fichiers temporaires
make clean

# Nettoyer compl√®tement (ATTENTION: supprime les donn√©es)
make clean-all
```

### Mise √† Jour
```bash
# Arr√™ter les services
make down

# Tirer les derni√®res modifications
git pull

# Mettre √† jour les d√©pendances
pip install -e ".[dev]" --upgrade

# Red√©marrer
make up
```

## Support

### Documentation
- **README**: Vue d'ensemble du projet
- **Architecture**: `/docs/architecture/`
- **Data Dictionary**: `/docs/data_dictionary/`
- **Runbooks**: `/docs/runbooks/`

### Ressources
- Email: support@sandtechnologies.bf
- Issues: GitHub Issues
- Documentation en ligne: https://docs.smi-platform.bf

### Logs
```bash
# Tous les services
make logs

# Service sp√©cifique
docker-compose logs -f <service-name>

# Fichiers de logs
ls -lh airflow/logs/
```

---

**Version**: 1.0.0  
**Derni√®re mise √† jour**: Janvier 2026  
**Maintenu par**: Sand Technologies - Healthcare Team
